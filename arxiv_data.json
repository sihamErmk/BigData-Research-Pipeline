[
{"source": "arXiv", "mot_cle_recherche": "Blockchain", "titre": "ChainSplitter: Towards Blockchain-based Industrial IoT Architecture for Supporting Hierarchical Storage", "lien": "http://arxiv.org/abs/1910.00742v1", "auteurs": ["Gang Wang", "Zhijie Jerry Shi", "Mark Nixon", "Song Han"], "annee": "2019", "abstract": "The fast developing Industrial Internet of Things (IIoT) technologies provide a promising opportunity to build large-scale systems to connect numerous heterogeneous devices into the Internet. Most existing IIoT infrastructures are based on a centralized architecture, which is easier for management but cannot effectively support immutable and verifiable services among multiple parties. Blockchain technology provides many desired features for large-scale IIoT infrastructures, such as decentralization, trustworthiness, trackability, and immutability. This paper presents a blockchain-based IIoT architecture to support immutable and verifiable services. However, when applying blockchain technology to the IIoT infrastructure, the required storage space posts a grant challenge to resource-constrained IIoT infrastructures. To address the storage issue, this paper proposes a hierarchical blockchain storage structure, \\textit{ChainSplitter}. Specially, the proposed architecture features a hierarchical storage structure where the majority of the blockchain is stored in the clouds, while the most recent blocks are stored in the overlay network of the individual IIoT networks. The proposed architecture seamlessly binds local IIoT networks, the blockchain overlay network, and the cloud infrastructure together through two connectors, the \\textit{blockchain connector} and the \\textit{cloud connector}, to construct the hierarchical blockchain storage. The blockchain connector in the overlay network builds blocks in blockchain from data generated in IIoT networks, and the cloud connector resolves the blockchain synchronization issues between the overlay network and the clouds. We also provide a case study to show the efficiency of the proposed hierarchical blockchain storage in a practical Industrial IoT case.", "journal": "arXiv", "date_scraping": "2026-01-14 14:18:52"},
{"source": "arXiv", "mot_cle_recherche": "Blockchain", "titre": "BlockSim: An Extensible Simulation Tool for Blockchain Systems", "lien": "http://arxiv.org/abs/2004.13438v2", "auteurs": ["Maher Alharby", "Aad van Moorsel"], "annee": "2020", "abstract": "Both in the design and deployment of blockchain solutions many performance-impacting configuration choices need to be made. We introduce BlockSim, a framework and software tool to build and simulate discrete-event dynamic systems models for blockchain systems. BlockSim is designed to support the analysis of a large variety of blockchains and blockchain deployments as well as a wide set of analysis questions. At the core of BlockSim is a Base Model, which contains the main model constructs common across various blockchain systems organized in three abstraction layers (network, consensus and incentives layer). The Base Model is usable for a wide variety of blockchain systems and can be extended easily to include system or deployment particulars. The BlockSim software tool provides a simulator that implements the Base Model in Python. This paper describes the Base Model, the simulator implementation, and the application of BlockSim to Bitcoin, Ethereum and other consensus algorithms. We validate BlockSim simulation results by comparison with performance results from actual systems and from other studies in the literature. We close the paper by a BlockSim simulation study of the impact of uncle blocks rewards on mining decentralization, for a variety of blockchain configurations.", "journal": "arXiv", "date_scraping": "2026-01-14 14:18:52"},
{"source": "arXiv", "mot_cle_recherche": "Blockchain", "titre": "SmartOil: Blockchain and smart contract-based oil supply chain management", "lien": "http://arxiv.org/abs/2105.05338v1", "auteurs": ["AKM Bahalul Haque", "Md. Rifat Hasan", "Md. Oahiduzzaman Mondol Zihad"], "annee": "2021", "abstract": "The traditional oil supply chain suffers from various shortcomings regarding crude oil extraction, processing, distribution, environmental pollution, and traceability. It offers an only a forward flow of products with almost no security and tracking process. In time, the system will lag behind due to the limitations in quality inspection, fraudulent information, and monopolistic behavior of supply chain entities. Inclusion of counterfeiting products and opaqueness of the system urge renovation in this sector. The recent evolution of Industry 4.0 leads to the alternation in the supply chain introducing the smart supply chain. Technological advancement can now reshape the infrastructure of the supply chain for the future. In this paper, we suggest a conceptual framework utilizing Blockchain and Smart Contract to monitor the overall oil supply chain. Blockchain is a groundbreaking technology to monitor and support the security building of a decentralized type supply chain over a peer-to-peer network. The use of the Internet of Things (IoT), especially sensors, opens broader window to track the global supply chain in real-time. We construct a methodology to support reverse traceability for each participant of the supply chain. The functions and characteristics of Blockchain and Smart Contract are defined. Implementation of Smart Contracts has also been shown with detailed analysis. We further describe the challenges of implementing such a system and validate our framework's adaptability in the real world. The paper concludes with future research scope to mitigate the restrictions of data management and maintenance with advanced working prototypes and agile systems achieving greater traceability and transparency.", "journal": "arXiv", "date_scraping": "2026-01-14 14:18:52"},
{"source": "arXiv", "mot_cle_recherche": "Blockchain", "titre": "Blockchain-Based Federated Learning: Incentivizing Data Sharing and Penalizing Dishonest Behavior", "lien": "http://arxiv.org/abs/2307.10492v1", "auteurs": ["Amir Jaberzadeh", "Ajay Kumar Shrestha", "Faijan Ahamad Khan", "Mohammed Afaan Shaikh", "Bhargav Dave", "Jason Geng"], "annee": "2023", "abstract": "With the increasing importance of data sharing for collaboration and innovation, it is becoming more important to ensure that data is managed and shared in a secure and trustworthy manner. Data governance is a common approach to managing data, but it faces many challenges such as data silos, data consistency, privacy, security, and access control. To address these challenges, this paper proposes a comprehensive framework that integrates data trust in federated learning with InterPlanetary File System, blockchain, and smart contracts to facilitate secure and mutually beneficial data sharing while providing incentives, access control mechanisms, and penalizing any dishonest behavior. The experimental results demonstrate that the proposed model is effective in improving the accuracy of federated learning models while ensuring the security and fairness of the data-sharing process. The research paper also presents a decentralized federated learning platform that successfully trained a CNN model on the MNIST dataset using blockchain technology. The platform enables multiple workers to train the model simultaneously while maintaining data privacy and security. The decentralized architecture and use of blockchain technology allow for efficient communication and coordination between workers. This platform has the potential to facilitate decentralized machine learning and support privacy-preserving collaboration in various domains.", "journal": "arXiv", "date_scraping": "2026-01-14 14:18:52"},
{"source": "arXiv", "mot_cle_recherche": "Blockchain", "titre": "Blockchain for Business Process Enactment: A Taxonomy and Systematic Literature Review", "lien": "http://arxiv.org/abs/2206.03237v2", "auteurs": ["Fabian Stiehle", "Ingo Weber"], "annee": "2022", "abstract": "Blockchain has been proposed to facilitate the enactment of interorganisational business processes. For such processes, blockchain can guarantee the enforcement of rules and the integrity of execution traces - without the need for a centralised trusted party. However, the enactment of interorganisational processes pose manifold challenges. In this work, we ask what answers the research field offers in response to those challenges. To do so, we conduct a systematic literature review (SLR). As our guiding question, we investigate the guarantees and capabilities of blockchain-based enactment approaches. Based on resulting empirical evidence, we develop a taxonomy for blockchain-based enactment. We find that a wide range of approaches support traceability and correctness; however, research focusing on flexibility and scalability remains nascent. For all challenges, we point towards future research opportunities.", "journal": "arXiv", "date_scraping": "2026-01-14 14:18:52"},
{"source": "arXiv", "mot_cle_recherche": "Blockchain", "titre": "A trustless society? A political look at the blockchain vision", "lien": "http://arxiv.org/abs/2405.06097v1", "auteurs": ["Rainer Rehak"], "annee": "2024", "abstract": "A lot of business and research effort currently deals with the so called decentralised ledger technology blockchain. Putting it to use carries the tempting promise to make the intermediaries of social interactions superfluous and furthermore keep secure track of all interactions. Currently intermediaries such as banks and notaries are necessary and must be trusted, which creates great dependencies, as the financial crisis of 2008 painfully demonstrated. Especially banks and notaries are said to become dispensable as a result of using the blockchain. But in real-world applications of the blockchain, the power of central actors does not dissolve, it only shifts to new, democratically illegitimate, uncontrolled or even uncontrollable power centers. As interesting as the blockchain technically is, it doesn't efficiently solve any real-world problem and is no substitute for traditional political processes or democratic regulation of power. Research efforts investigating the blockchain should be halted.", "journal": "arXiv", "date_scraping": "2026-01-14 14:18:52"},
{"source": "arXiv", "mot_cle_recherche": "Blockchain", "titre": "Oceanic Games: Centralization Risks and Incentives in Blockchain Mining", "lien": "http://arxiv.org/abs/1904.02368v4", "auteurs": ["Nikos Leonardos", "Stefanos Leonardos", "Georgios Piliouras"], "annee": "2019", "abstract": "To participate in the distributed consensus of permissionless blockchains, prospective nodes -- or miners -- provide proof of designated, costly resources. However, in contrast to the intended decentralization, current data on blockchain mining unveils increased concentration of these resources in a few major entities, typically mining pools. To study strategic considerations in this setting, we employ the concept of Oceanic Games, Milnor and Shapley (1978). Oceanic Games have been used to analyze decision making in corporate settings with small numbers of dominant players (shareholders) and large numbers of individually insignificant players, the ocean. Unlike standard equilibrium models, they focus on measuring the value (or power) per entity and per unit of resource} in a given distribution of resources. These values are viewed as strategic components in coalition formations, mergers and resource acquisitions. Considering such issues relevant to blockchain governance and long-term sustainability, we adapt oceanic games to blockchain mining and illustrate the defined concepts via examples. The application of existing results reveals incentives for individual miners to merge in order to increase the value of their resources. This offers an alternative perspective to the observed centralization and concentration of mining power. Beyond numerical simulations, we use the model to identify issues relevant to the design of future cryptocurrencies and formulate prospective research questions.", "journal": "arXiv", "date_scraping": "2026-01-14 14:18:52"},
{"source": "arXiv", "mot_cle_recherche": "Blockchain", "titre": "Customer Data Sharing Platform: A Blockchain-Based Shopping Cart", "lien": "http://arxiv.org/abs/2004.13890v1", "auteurs": ["Ajay Kumar Shrestha", "Sandhya Joshi", "Julita Vassileva"], "annee": "2020", "abstract": "We propose a new free eCommerce platform with blockchains that allows customers to connect to the seller directly, share personal data without losing control and ownership of it and apply it to the domain of shopping cart. Our new platform provides a solution to four important problems: private payment, ensuring privacy and user control, and incentives for sharing. It allows the trade to be open, transparent with immutable transactions that can be used for settling any disputes. The paper presents a case study of applying the framework for a shopping cart as one of the enterprise nodes of MultiChain which provides trading in ethers controlled by smart contracts and also collects user profile data and allows them to receive rewards for sharing their data with other business enterprises. It tracks who shared what, with whom, when, by what means and for what purposes in a verifiable fashion. The user data from the repository is converted into an open data format and shared via stream in the blockchain so that other nodes can efficiently process and use the data. The smart contract verifies and executes the agreed terms of use of the data and transfers digital tokens as a reward to the customer. The smart contract imposes double deposit collateral to ensure that all participants act honestly.", "journal": "arXiv", "date_scraping": "2026-01-14 14:18:52"},
{"source": "arXiv", "mot_cle_recherche": "Blockchain", "titre": "Advanced Drone Swarm Security by Using Blockchain Governance Game", "lien": "http://arxiv.org/abs/2112.15454v4", "auteurs": ["Song-Kyoo Kim"], "annee": "2021", "abstract": "This research contributes to the security design of an advanced smart drone swarm network based on a variant of the Blockchain Governance Game (BGG), which is the theoretical game model to predict the moments of security actions before attacks, and the Strategic Alliance for Blockchain Governance Game (SABGG), which is one of the BGG variants which has been adapted to construct the best strategies to take preliminary actions based on strategic alliance for protecting smart drones in a blockchain-based swarm network. Smart drones are artificial intelligence (AI)-enabled drones which are capable of being operated autonomously without having any command center. Analytically tractable solutions from the SABGG allow us to estimate the moments of taking preliminary actions by delivering the optimal accountability of drones for preventing attacks. This advanced secured swarm network within AI-enabled drones is designed by adapting the SABGG model. This research helps users to develop a new network-architecture-level security of a smart drone swarm which is based on a decentralized network.", "journal": "arXiv", "date_scraping": "2026-01-14 14:18:52"},
{"source": "arXiv", "mot_cle_recherche": "Blockchain", "titre": "Digital Twins and Blockchain for IoT Management", "lien": "http://arxiv.org/abs/2309.01042v1", "auteurs": ["Mayra Samaniego", "Ralph Deters"], "annee": "2023", "abstract": "Security and privacy are primary concerns in IoT management. Security breaches in IoT resources, such as smart sensors, can leak sensitive data and compromise the privacy of individuals. Effective IoT management requires a comprehensive approach to prioritize access security and data privacy protection. Digital twins create virtual representations of IoT resources. Blockchain adds decentralization, transparency, and reliability to IoT systems. This research integrates digital twins and blockchain to manage access to IoT data streaming. Digital twins are used to encapsulate data access and view configurations. Access is enabled on digital twins, not on IoT resources directly. Trust structures programmed as smart contracts are the ones that manage access to digital twins. Consequently, IoT resources are not exposed to third parties, and access security breaches can be prevented. Blockchain has been used to validate digital twins and store their configuration. The research presented in this paper enables multitenant access and customization of data streaming views and abstracts the complexity of data access management. This approach provides access and configuration security and data privacy protection.", "journal": "arXiv", "date_scraping": "2026-01-14 14:18:52"},
{"source": "arXiv", "mot_cle_recherche": "Deep Learning", "titre": "Learn to Accumulate Evidence from All Training Samples: Theory and Practice", "lien": "http://arxiv.org/abs/2306.11113v2", "auteurs": ["Deep Pandey", "Qi Yu"], "annee": "2023", "abstract": "Evidential deep learning, built upon belief theory and subjective logic, offers a principled and computationally efficient way to turn a deterministic neural network uncertainty-aware. The resultant evidential models can quantify fine-grained uncertainty using the learned evidence. To ensure theoretically sound evidential models, the evidence needs to be non-negative, which requires special activation functions for model training and inference. This constraint often leads to inferior predictive performance compared to standard softmax models, making it challenging to extend them to many large-scale datasets. To unveil the real cause of this undesired behavior, we theoretically investigate evidential models and identify a fundamental limitation that explains the inferior performance: existing evidential activation functions create zero evidence regions, which prevent the model to learn from training samples falling into such regions. A deeper analysis of evidential activation functions based on our theoretical underpinning inspires the design of a novel regularizer that effectively alleviates this fundamental limitation. Extensive experiments over many challenging real-world datasets and settings confirm our theoretical findings and demonstrate the effectiveness of our proposed approach.", "journal": "arXiv", "date_scraping": "2026-01-14 14:18:55"},
{"source": "arXiv", "mot_cle_recherche": "Deep Learning", "titre": "The Modern Mathematics of Deep Learning", "lien": "http://arxiv.org/abs/2105.04026v2", "auteurs": ["Julius Berner", "Philipp Grohs", "Gitta Kutyniok", "Philipp Petersen"], "annee": "2021", "abstract": "We describe the new field of mathematical analysis of deep learning. This field emerged around a list of research questions that were not answered within the classical framework of learning theory. These questions concern: the outstanding generalization power of overparametrized neural networks, the role of depth in deep architectures, the apparent absence of the curse of dimensionality, the surprisingly successful optimization performance despite the non-convexity of the problem, understanding what features are learned, why deep architectures perform exceptionally well in physical problems, and which fine aspects of an architecture affect the behavior of a learning task in which way. We present an overview of modern approaches that yield partial answers to these questions. For selected approaches, we describe the main ideas in more detail.", "journal": "arXiv", "date_scraping": "2026-01-14 14:18:55"},
{"source": "arXiv", "mot_cle_recherche": "Deep Learning", "titre": "Deep Learning and Computational Physics (Lecture Notes)", "lien": "http://arxiv.org/abs/2301.00942v1", "auteurs": ["Deep Ray", "Orazio Pinti", "Assad A. Oberai"], "annee": "2023", "abstract": "These notes were compiled as lecture notes for a course developed and taught at the University of the Southern California. They should be accessible to a typical engineering graduate student with a strong background in Applied Mathematics.\n  The main objective of these notes is to introduce a student who is familiar with concepts in linear algebra and partial differential equations to select topics in deep learning. These lecture notes exploit the strong connections between deep learning algorithms and the more conventional techniques of computational physics to achieve two goals. First, they use concepts from computational physics to develop an understanding of deep learning algorithms. Not surprisingly, many concepts in deep learning can be connected to similar concepts in computational physics, and one can utilize this connection to better understand these algorithms. Second, several novel deep learning algorithms can be used to solve challenging problems in computational physics. Thus, they offer someone who is interested in modeling a physical phenomena with a complementary set of tools.", "journal": "arXiv", "date_scraping": "2026-01-14 14:18:55"},
{"source": "arXiv", "mot_cle_recherche": "Deep Learning", "titre": "A multitask deep learning model for real-time deployment in embedded systems", "lien": "http://arxiv.org/abs/1711.00146v1", "auteurs": ["Miquel Martí", "Atsuto Maki"], "annee": "2017", "abstract": "We propose an approach to Multitask Learning (MTL) to make deep learning models faster and lighter for applications in which multiple tasks need to be solved simultaneously, which is particularly useful in embedded, real-time systems. We develop a multitask model for both Object Detection and Semantic Segmentation and analyze the challenges that appear during its training. Our multitask network is 1.6x faster, lighter and uses less memory than deploying the single-task models in parallel. We conclude that MTL has the potential to give superior performance in exchange of a more complex training process that introduces challenges not present in single-task models.", "journal": "arXiv", "date_scraping": "2026-01-14 14:18:55"},
{"source": "arXiv", "mot_cle_recherche": "Deep Learning", "titre": "DILIE: Deep Internal Learning for Image Enhancement", "lien": "http://arxiv.org/abs/2012.06469v1", "auteurs": ["Indra Deep Mastan", "Shanmuganathan Raman"], "annee": "2020", "abstract": "We consider the generic deep image enhancement problem where an input image is transformed into a perceptually better-looking image. Recent methods for image enhancement consider the problem by performing style transfer and image restoration. The methods mostly fall into two categories: training data-based and training data-independent (deep internal learning methods). We perform image enhancement in the deep internal learning framework. Our Deep Internal Learning for Image Enhancement framework enhances content features and style features and uses contextual content loss for preserving image context in the enhanced image. We show results on both hazy and noisy image enhancement. To validate the results, we use structure similarity and perceptual error, which is efficient in measuring the unrealistic deformation present in the images. We show that the proposed framework outperforms the relevant state-of-the-art works for image enhancement.", "journal": "arXiv", "date_scraping": "2026-01-14 14:18:55"},
{"source": "arXiv", "mot_cle_recherche": "Deep Learning", "titre": "Deep learning observables in computational fluid dynamics", "lien": "http://arxiv.org/abs/1903.03040v2", "auteurs": ["Kjetil O. Lye", "Siddhartha Mishra", "Deep Ray"], "annee": "2019", "abstract": "Many large scale problems in computational fluid dynamics such as uncertainty quantification, Bayesian inversion, data assimilation and PDE constrained optimization are considered very challenging computationally as they require a large number of expensive (forward) numerical solutions of the corresponding PDEs. We propose a machine learning algorithm, based on deep artificial neural networks, that predicts the underlying \\emph{input parameters to observable} map from a few training samples (computed realizations of this map). By a judicious combination of theoretical arguments and empirical observations, we find suitable network architectures and training hyperparameters that result in robust and efficient neural network approximations of the parameters to observable map. Numerical experiments are presented to demonstrate low prediction errors for the trained network networks, even when the network has been trained with a few samples, at a computational cost which is several orders of magnitude lower than the underlying PDE solver.\n  Moreover, we combine the proposed deep learning algorithm with Monte Carlo (MC) and Quasi-Monte Carlo (QMC) methods to efficiently compute uncertainty propagation for nonlinear PDEs. Under the assumption that the underlying neural networks generalize well, we prove that the deep learning MC and QMC algorithms are guaranteed to be faster than the baseline (quasi-) Monte Carlo methods. Numerical experiments demonstrating one to two orders of magnitude speed up over baseline QMC and MC algorithms, for the intricate problem of computing probability distributions of the observable, are also presented.", "journal": "arXiv", "date_scraping": "2026-01-14 14:18:55"},
{"source": "arXiv", "mot_cle_recherche": "Deep Learning", "titre": "Generalized Regularized Evidential Deep Learning Models: Theory and Comprehensive Evaluation", "lien": "http://arxiv.org/abs/2512.23753v1", "auteurs": ["Deep Shankar Pandey", "Hyomin Choi", "Qi Yu"], "annee": "2025", "abstract": "Evidential deep learning (EDL) models, based on Subjective Logic, introduce a principled and computationally efficient way to make deterministic neural networks uncertainty-aware. The resulting evidential models can quantify fine-grained uncertainty using learned evidence. However, the Subjective-Logic framework constrains evidence to be non-negative, requiring specific activation functions whose geometric properties can induce activation-dependent learning-freeze behavior: a regime where gradients become extremely small for samples mapped into low-evidence regions. We theoretically characterize this behavior and analyze how different evidential activations influence learning dynamics. Building on this analysis, we design a general family of activation functions and corresponding evidential regularizers that provide an alternative pathway for consistent evidence updates across activation regimes. Extensive experiments on four benchmark classification problems (MNIST, CIFAR-10, CIFAR-100, and Tiny-ImageNet), two few-shot classification problems, and blind face restoration problem empirically validate the developed theory and demonstrate the effectiveness of the proposed generalized regularized evidential models.", "journal": "arXiv", "date_scraping": "2026-01-14 14:18:55"},
{"source": "arXiv", "mot_cle_recherche": "Deep Learning", "titre": "Activation Analysis of a Byte-Based Deep Neural Network for Malware Classification", "lien": "http://arxiv.org/abs/1903.04717v2", "auteurs": ["Scott E. Coull", "Christopher Gardner"], "annee": "2019", "abstract": "Feature engineering is one of the most costly aspects of developing effective machine learning models, and that cost is even greater in specialized problem domains, like malware classification, where expert skills are necessary to identify useful features. Recent work, however, has shown that deep learning models can be used to automatically learn feature representations directly from the raw, unstructured bytes of the binaries themselves. In this paper, we explore what these models are learning about malware. To do so, we examine the learned features at multiple levels of resolution, from individual byte embeddings to end-to-end analysis of the model. At each step, we connect these byte-oriented activations to their original semantics through parsing and disassembly of the binary to arrive at human-understandable features. Through our results, we identify several interesting features learned by the model and their connection to manually-derived features typically used by traditional machine learning models. Additionally, we explore the impact of training data volume and regularization on the quality of the learned features and the efficacy of the classifiers, revealing the somewhat paradoxical insight that better generalization does not necessarily result in better performance for byte-based malware classifiers.", "journal": "arXiv", "date_scraping": "2026-01-14 14:18:55"},
{"source": "arXiv", "mot_cle_recherche": "Deep Learning", "titre": "Predicting Thrombectomy Recanalization from CT Imaging Using Deep Learning Models", "lien": "http://arxiv.org/abs/2302.04143v2", "auteurs": ["Haoyue Zhang", "Jennifer S. Polson", "Eric J. Yang", "Kambiz Nael", "William Speier", "Corey W. Arnold"], "annee": "2023", "abstract": "For acute ischemic stroke (AIS) patients with large vessel occlusions, clinicians must decide if the benefit of mechanical thrombectomy (MTB) outweighs the risks and potential complications following an invasive procedure. Pre-treatment computed tomography (CT) and angiography (CTA) are widely used to characterize occlusions in the brain vasculature. If a patient is deemed eligible, a modified treatment in cerebral ischemia (mTICI) score will be used to grade how well blood flow is reestablished throughout and following the MTB procedure. An estimation of the likelihood of successful recanalization can support treatment decision-making. In this study, we proposed a fully automated prediction of a patient's recanalization score using pre-treatment CT and CTA imaging. We designed a spatial cross attention network (SCANet) that utilizes vision transformers to localize to pertinent slices and brain regions. Our top model achieved an average cross-validated ROC-AUC of 77.33 $\\pm$ 3.9\\%. This is a promising result that supports future applications of deep learning on CT and CTA for the identification of eligible AIS patients for MTB.", "journal": "arXiv", "date_scraping": "2026-01-14 14:18:55"},
{"source": "arXiv", "mot_cle_recherche": "Deep Learning", "titre": "DeepCFL: Deep Contextual Features Learning from a Single Image", "lien": "http://arxiv.org/abs/2011.03712v1", "auteurs": ["Indra Deep Mastan", "Shanmuganathan Raman"], "annee": "2020", "abstract": "Recently, there is a vast interest in developing image feature learning methods that are independent of the training data, such as deep image prior, InGAN, SinGAN, and DCIL. These methods are unsupervised and are used to perform low-level vision tasks such as image restoration, image editing, and image synthesis. In this work, we proposed a new training data-independent framework, called Deep Contextual Features Learning (DeepCFL), to perform image synthesis and image restoration based on the semantics of the input image. The contextual features are simply the high dimensional vectors representing the semantics of the given image. DeepCFL is a single image GAN framework that learns the distribution of the context vectors from the input image. We show the performance of contextual learning in various challenging scenarios: outpainting, inpainting, and restoration of randomly removed pixels. DeepCFL is applicable when the input source image and the generated target image are not aligned. We illustrate image synthesis using DeepCFL for the task of image resizing.", "journal": "arXiv", "date_scraping": "2026-01-14 14:18:55"},
{"source": "arXiv", "mot_cle_recherche": "Big Data", "titre": "Big Data Visualization Tools", "lien": "http://arxiv.org/abs/1801.08336v3", "auteurs": ["Nikos Bikakis"], "annee": "2018", "abstract": "Data visualization and analytics are nowadays one of the corner-stones of Data Science, turning the abundance of Big Data being produced through modern systems into actionable knowledge. Indeed, the Big Data era has realized the availability of voluminous datasets that are dynamic, noisy and heterogeneous in nature. Transforming a data-curious user into someone who can access and analyze that data is even more burdensome now for a great number of users with little or no support and expertise on the data processing part. Thus, the area of data visualization and analysis has gained great attention recently, calling for joint action from different research areas and communities such as information visualization, data management and mining, human-computer interaction, and computer graphics. This article presents the limitations of traditional visualization systems in the Big Data era. Additionally, it discusses the major prerequisites and challenges that should be addressed by modern visualization systems. Finally, the state-of-the-art methods that have been developed in the context of the Big Data visualization and analytics are presented, considering methods from the Data Management and Mining, Information Visualization and Human-Computer Interaction communities", "journal": "arXiv", "date_scraping": "2026-01-14 14:18:56"},
{"source": "arXiv", "mot_cle_recherche": "Big Data", "titre": "Big Data and Fog Computing", "lien": "http://arxiv.org/abs/1712.09552v1", "auteurs": ["Yogesh Simmhan"], "annee": "2017", "abstract": "Fog computing serves as a computing layer that sits between the edge devices and the cloud in the network topology. They have more compute capacity than the edge but much less so than cloud data centers. They typically have high uptime and always-on Internet connectivity. Applications that make use of the fog can avoid the network performance limitation of cloud computing while being less resource constrained than edge computing. As a result, they offer a useful balance of the current paradigms. This article explores various aspects of fog computing in the context of big data.", "journal": "arXiv", "date_scraping": "2026-01-14 14:18:56"},
{"source": "arXiv", "mot_cle_recherche": "Big Data", "titre": "Big Data Analytics for Dynamic Energy Management in Smart Grids", "lien": "http://arxiv.org/abs/1504.02424v3", "auteurs": ["Panagiotis D. Diamantoulakis", "Vasileios M. Kapinas", "George K. Karagiannidis"], "annee": "2015", "abstract": "The smart electricity grid enables a two-way flow of power and data between suppliers and consumers in order to facilitate the power flow optimization in terms of economic efficiency, reliability and sustainability. This infrastructure permits the consumers and the micro-energy producers to take a more active role in the electricity market and the dynamic energy management (DEM). The most important challenge in a smart grid (SG) is how to take advantage of the users' participation in order to reduce the cost of power. However, effective DEM depends critically on load and renewable production forecasting. This calls for intelligent methods and solutions for the real-time exploitation of the large volumes of data generated by a vast amount of smart meters. Hence, robust data analytics, high performance computing, efficient data network management, and cloud computing techniques are critical towards the optimized operation of SGs. This research aims to highlight the big data issues and challenges faced by the DEM employed in SG networks. It also provides a brief description of the most commonly used data processing methods in the literature, and proposes a promising direction for future research in the field.", "journal": "arXiv", "date_scraping": "2026-01-14 14:18:56"},
{"source": "arXiv", "mot_cle_recherche": "Big Data", "titre": "A Semantic Approach for Big Data Exploration in Industry 4.0", "lien": "http://arxiv.org/abs/2401.09789v1", "auteurs": ["Idoia Berges", "Víctor Julio Ramírez-Durán", "Arantza Illarramendi"], "annee": "2024", "abstract": "The growing trends in automation, Internet of Things, big data and cloud computing technologies have led to the fourth industrial revolution (Industry 4.0), where it is possible to visualize and identify patterns and insights, which results in a better understanding of the data and can improve the manufacturing process. However, many times, the task of data exploration results difficult for manufacturing experts because they might be interested in analyzing also data that does not appear in pre-designed visualizations and therefore they must be assisted by Information Technology experts. In this paper, we present a proposal materialized in a semantic-based visual query system developed for a real Industry 4.0 scenario that allows domain experts to explore and visualize data in a friendly way. The main novelty of the system is the combined use that it makes of captured data that are semantically annotated first, and a 2D customized digital representation of a machine that is also linked with semantic descriptions. Those descriptions are expressed using terms of an ontology, where, among others, the sensors that are used to capture indicators about the performance of a machine that belongs to a Industry 4.0 scenario have been modeled. Moreover, this semantic description allows to: formulate queries at a higher level of abstraction, provide customized graphical visualizations of the results based on the format and nature of the data, and download enriched data enabling further types of analysis.", "journal": "arXiv", "date_scraping": "2026-01-14 14:18:56"},
{"source": "arXiv", "mot_cle_recherche": "Big Data", "titre": "Building the National Radio Recordings Database: A Big Data Approach to Documenting Audio Heritage", "lien": "http://arxiv.org/abs/1911.04625v1", "auteurs": ["Emily Goodmann", "Mark A. Matienzo", "Shawn VanCour", "William Vanden Dries"], "annee": "2019", "abstract": "This paper traces strategies used by the Radio Preservation Task Force of the Library of Congress's National Recording Preservation Board to develop a publicly searchable database documenting extant radio materials held by collecting institutions throughout the country. Having aggregated metadata on 2,500 unique collections to date, the project has encountered a series of logistical challenges that are not only technical in nature but also institutional and social, raising critical issues involving organizational structure, political representation, and the ethics of data access. As the project continues to expand and evolve, lessons from its early development offer valuable reminders of the human judgment, hidden labor, and interpersonal relations required for successful big data work.", "journal": "arXiv", "date_scraping": "2026-01-14 14:18:56"},
{"source": "arXiv", "mot_cle_recherche": "Big Data", "titre": "Byzantine-Resilient SGD in High Dimensions on Heterogeneous Data", "lien": "http://arxiv.org/abs/2005.07866v1", "auteurs": ["Deepesh Data", "Suhas Diggavi"], "annee": "2020", "abstract": "We study distributed stochastic gradient descent (SGD) in the master-worker architecture under Byzantine attacks. We consider the heterogeneous data model, where different workers may have different local datasets, and we do not make any probabilistic assumptions on data generation. At the core of our algorithm, we use the polynomial-time outlier-filtering procedure for robust mean estimation proposed by Steinhardt et al. (ITCS 2018) to filter-out corrupt gradients. In order to be able to apply their filtering procedure in our {\\em heterogeneous} data setting where workers compute {\\em stochastic} gradients, we derive a new matrix concentration result, which may be of independent interest.\n  We provide convergence analyses for smooth strongly-convex and non-convex objectives. We derive our results under the bounded variance assumption on local stochastic gradients and a {\\em deterministic} condition on datasets, namely, gradient dissimilarity; and for both these quantities, we provide concrete bounds in the statistical heterogeneous data model. We give a trade-off between the mini-batch size for stochastic gradients and the approximation error. Our algorithm can tolerate up to $\\frac{1}{4}$ fraction Byzantine workers. It can find approximate optimal parameters in the strongly-convex setting exponentially fast and reach to an approximate stationary point in the non-convex setting with a linear speed, thus, matching the convergence rates of vanilla SGD in the Byzantine-free setting.\n  We also propose and analyze a Byzantine-resilient SGD algorithm with gradient compression, where workers send $k$ random coordinates of their gradients. Under mild conditions, we show a $\\frac{d}{k}$-factor saving in communication bits as well as decoding complexity over our compression-free algorithm without affecting its convergence rate (order-wise) and the approximation error.", "journal": "arXiv", "date_scraping": "2026-01-14 14:18:56"},
{"source": "arXiv", "mot_cle_recherche": "Big Data", "titre": "Data Encoding for Byzantine-Resilient Distributed Optimization", "lien": "http://arxiv.org/abs/1907.02664v2", "auteurs": ["Deepesh Data", "Linqi Song", "Suhas Diggavi"], "annee": "2019", "abstract": "We study distributed optimization in the presence of Byzantine adversaries, where both data and computation are distributed among $m$ worker machines, $t$ of which may be corrupt. The compromised nodes may collaboratively and arbitrarily deviate from their pre-specified programs, and a designated (master) node iteratively computes the model/parameter vector for generalized linear models. In this work, we primarily focus on two iterative algorithms: Proximal Gradient Descent (PGD) and Coordinate Descent (CD). Gradient descent (GD) is a special case of these algorithms. PGD is typically used in the data-parallel setting, where data is partitioned across different samples, whereas, CD is used in the model-parallelism setting, where data is partitioned across the parameter space.\n  In this paper, we propose a method based on data encoding and error correction over real numbers to combat adversarial attacks. We can tolerate up to $t\\leq \\lfloor\\frac{m-1}{2}\\rfloor$ corrupt worker nodes, which is information-theoretically optimal. We give deterministic guarantees, and our method does not assume any probability distribution on the data. We develop a {\\em sparse} encoding scheme which enables computationally efficient data encoding and decoding. We demonstrate a trade-off between the corruption threshold and the resource requirements (storage, computational, and communication complexity). As an example, for $t\\leq\\frac{m}{3}$, our scheme incurs only a {\\em constant} overhead on these resources, over that required by the plain distributed PGD/CD algorithms which provide no adversarial protection. To the best of our knowledge, ours is the first paper that makes CD secure against adversarial attacks.\n  Our encoding scheme extends efficiently to the data streaming model and for stochastic gradient descent (SGD). We also give experimental results to show the efficacy of our proposed schemes.", "journal": "arXiv", "date_scraping": "2026-01-14 14:18:56"},
{"source": "arXiv", "mot_cle_recherche": "Big Data", "titre": "Big Data: Understanding Big Data", "lien": "http://arxiv.org/abs/1601.04602v1", "auteurs": ["Kevin Taylor-Sakyi"], "annee": "2016", "abstract": "Steve Jobs, one of the greatest visionaries of our time was quoted in 1996 saying \"a lot of times, people do not know what they want until you show it to them\" [38] indicating he advocated products to be developed based on human intuition rather than research. With the advancements of mobile devices, social networks and the Internet of Things, enormous amounts of complex data, both structured and unstructured are being captured in hope to allow organizations to make better business decisions as data is now vital for an organizations success. These enormous amounts of data are referred to as Big Data, which enables a competitive advantage over rivals when processed and analyzed appropriately. However Big Data Analytics has a few concerns including Management of Data-lifecycle, Privacy & Security, and Data Representation. This paper reviews the fundamental concept of Big Data, the Data Storage domain, the MapReduce programming paradigm used in processing these large datasets, and focuses on two case studies showing the effectiveness of Big Data Analytics and presents how it could be of greater good in the future if handled appropriately.", "journal": "arXiv", "date_scraping": "2026-01-14 14:18:56"},
{"source": "arXiv", "mot_cle_recherche": "Big Data", "titre": "Constraints on dark energy from H II starburst galaxy apparent magnitude versus redshift data", "lien": "http://arxiv.org/abs/1110.5626v1", "auteurs": ["Data Mania", "Bharat Ratra"], "annee": "2011", "abstract": "In this paper we use H II starburst galaxy apparent magnitude versus redshift data from Siegel et al. (2005) to constrain dark energy cosmological model parameters. These constraints are generally consistent with those derived using other data sets, but are not as restrictive as the tightest currently available constraints.", "journal": "arXiv", "date_scraping": "2026-01-14 14:18:56"},
{"source": "arXiv", "mot_cle_recherche": "Big Data", "titre": "Big Data", "lien": "http://arxiv.org/abs/2012.09109v1", "auteurs": ["Andreas L Opdahl", "Vimala Nunavath"], "annee": "2020", "abstract": "The Internet of Things, crowdsourcing, social media, public authorities, and other sources generate bigger and bigger data sets. Big and open data offers many benefits for emergency management, but also pose new challenges. This chapter will review the sources of big data and their characteristics. We then discuss potential benefits of big data for emergency management along with the technological and societal challenges it poses. We review central technologies for big-data storage and processing in general, before presenting the Spark big-data engine in more detail. Finally, we review ethical and societal threats that big data pose.", "journal": "arXiv", "date_scraping": "2026-01-14 14:18:56"}
]